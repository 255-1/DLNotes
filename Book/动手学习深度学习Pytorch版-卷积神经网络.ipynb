{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d11c1b52",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T08:08:16.186381Z",
     "start_time": "2021-12-06T08:08:16.178403Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import time\n",
    "import torchvision\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79863f66",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T07:20:24.149574Z",
     "start_time": "2021-12-06T07:20:24.129624Z"
    }
   },
   "outputs": [],
   "source": [
    "def corr2d(X, K):  # 本函数已保存在d2lzh_pytorch包中方便以后使用\n",
    "    h, w = K.shape\n",
    "    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            Y[i, j] = (X[i: i + h, j: j + w] * K).sum()\n",
    "    return Y\n",
    "\n",
    "#对x的形状转换,把图片拉成一个向量   \n",
    "class FlattenLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FlattenLayer, self).__init__()\n",
    "    def forward(self, x):\n",
    "        return x.view(x.shape[0],-1)\n",
    "    \n",
    "def load_data_fashion_mnist(batch_size, resize=None, root='~/Datasets/FashionMNIST'):\n",
    "    \"\"\"Download the fashion mnist dataset and then load into memory.\"\"\"\n",
    "    trans = []\n",
    "    if resize:\n",
    "        trans.append(torchvision.transforms.Resize(size=resize))\n",
    "    trans.append(torchvision.transforms.ToTensor())\n",
    "\n",
    "    transform = torchvision.transforms.Compose(trans)\n",
    "    mnist_train = torchvision.datasets.FashionMNIST(root=root, train=True, download=True, transform=transform)\n",
    "    mnist_test = torchvision.datasets.FashionMNIST(root=root, train=False, download=True, transform=transform)\n",
    "\n",
    "    train_iter = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True)\n",
    "    test_iter = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_iter, test_iter\n",
    "\n",
    "def evaluate_accuracy(data_iter, net, device=None):\n",
    "    if device is None and isinstance(net, torch.nn.Module):\n",
    "        # 如果没指定device就使用net的device\n",
    "        device = list(net.parameters())[0].device\n",
    "    acc_sum, n = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            if isinstance(net, torch.nn.Module):\n",
    "                net.eval() # 评估模式, 这会关闭dropout\n",
    "                acc_sum += (net(X.to(device)).argmax(dim=1) == y.to(device)).float().sum().cpu().item()\n",
    "                net.train() # 改回训练模式\n",
    "            else: # 自定义的模型, 3.13节之后不会用到, 不考虑GPU\n",
    "                if('is_training' in net.__code__.co_varnames): # 如果有is_training这个参数\n",
    "                    # 将is_training设置成False\n",
    "                    acc_sum += (net(X, is_training=False).argmax(dim=1) == y).float().sum().item() \n",
    "                else:\n",
    "                    acc_sum += (net(X).argmax(dim=1) == y).float().sum().item() \n",
    "            n += y.shape[0]\n",
    "    return acc_sum / n\n",
    "\n",
    "def train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs):\n",
    "    net = net.to(device)\n",
    "    print(\"training on \", device)\n",
    "    loss = torch.nn.CrossEntropyLoss()\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n, batch_count, start = 0.0, 0.0, 0, 0, time.time()\n",
    "        for X, y in train_iter:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y)\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            train_l_sum += l.cpu().item()\n",
    "            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().cpu().item()\n",
    "            n += y.shape[0]\n",
    "            batch_count += 1\n",
    "        test_acc = evaluate_accuracy(test_iter, net)\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec'\n",
    "              % (epoch + 1, train_l_sum / batch_count, train_acc_sum / n, test_acc, time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c48454",
   "metadata": {},
   "source": [
    "# 二维卷积层"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100e36b2",
   "metadata": {},
   "source": [
    "## 二维互相关运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8646ac8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T05:41:01.971632Z",
     "start_time": "2021-12-05T05:41:01.957667Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[19., 25.],\n",
       "        [37., 43.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([[0, 1, 2], [3, 4, 5], [6, 7, 8]])\n",
    "K = torch.tensor([[0, 1], [2, 3]])\n",
    "corr2d(X, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ceba02b",
   "metadata": {},
   "source": [
    "## 二维卷积层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9567d0b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T05:41:01.986611Z",
     "start_time": "2021-12-05T05:41:01.972626Z"
    }
   },
   "outputs": [],
   "source": [
    "#二维卷积层将输入和卷积核做互相关运算，并加上一个标量偏差来得到输出\n",
    "class Conv2D(nn.Module):\n",
    "    def __init__(self, kernel_size):\n",
    "        super(Conv2D, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(kernel_size))\n",
    "        self.bias = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return corr2d(x, self.weight) + self.bias\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21afb667",
   "metadata": {},
   "source": [
    "## 图像中物体边缘检测\n",
    "检测图像中物体的边缘，即找到像素变化的位置。首先我们构造一张6×86×8的图像（即高和宽分别为6像素和8像素的图像）。它中间4列为黑（0），其余为白（1）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cea7efca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T05:41:02.001553Z",
     "start_time": "2021-12-05T05:41:01.987591Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.ones(6, 8)\n",
    "X[:, 2:6] = 0\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f838a995",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T05:41:02.017510Z",
     "start_time": "2021-12-05T05:41:02.002551Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#构造一个高和宽分别为1和2的卷积核K。\n",
    "#当它与输入做互相关运算时，如果横向相邻元素相同，输出为0；否则输出为非0。\n",
    "K = torch.tensor([[1, -1]])\n",
    "#卷积层可通过重复使用卷积核有效地表征局部空间\n",
    "Y = corr2d(X, K)\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28aa2cbe",
   "metadata": {},
   "source": [
    "## 通过数据学习核数组\n",
    "网页版无法运行，使用官方教材内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eddbed49",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T05:41:02.048462Z",
     "start_time": "2021-12-05T05:41:02.018508Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5, loss 0.911\n",
      "Step 10, loss 0.024\n",
      "Step 15, loss 0.002\n",
      "Step 20, loss 0.000\n"
     ]
    }
   ],
   "source": [
    "conv2d = nn.Conv2d(1,1,kernel_size=(1,2),bias=False)\n",
    "X = X.reshape((1,1,6,8))\n",
    "Y = Y.reshape((1,1,6,7))\n",
    "lr=3e-2\n",
    "for i in range(20):\n",
    "    Y_hat = conv2d(X)\n",
    "    l = (Y_hat - Y) ** 2\n",
    "    conv2d.zero_grad()\n",
    "    l.sum().backward()\n",
    "    conv2d.weight.data[:] -= lr*conv2d.weight.grad\n",
    "    if (i + 1) % 5 == 0:\n",
    "        print('Step %d, loss %.3f' % (i + 1, l.sum().item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0fb9364",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T05:41:02.064386Z",
     "start_time": "2021-12-05T05:41:02.050421Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight:  tensor([[[[ 1.0012, -0.9984]]]])\n"
     ]
    }
   ],
   "source": [
    "print(\"weight: \", conv2d.weight.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de83c3b",
   "metadata": {},
   "source": [
    "# 填充和步幅"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfba836",
   "metadata": {},
   "source": [
    "## 填充\n",
    "我们会设置(拓宽数=核大小-1)来使输入和输出具有相同的高和宽。这样会方便在构造网络时推测每个层的输出形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df82d255",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T05:41:02.080342Z",
     "start_time": "2021-12-05T05:41:02.065383Z"
    }
   },
   "outputs": [],
   "source": [
    "# 定义一个函数来计算卷积层。它对输入和输出做相应的升维和降维\n",
    "def comp_conv2d(conv2d, X):\n",
    "    # (1, 1)代表批量大小和通道数均为1\n",
    "    X = X.view((1, 1) + X.shape)\n",
    "    Y = conv2d(X)\n",
    "    return Y.view(Y.shape[2:])  # 排除不关心的前两维：批量和通道"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e58311a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T05:41:02.111764Z",
     "start_time": "2021-12-05T05:41:02.081339Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, padding=1)\n",
    "\n",
    "X = torch.rand(8, 8)\n",
    "comp_conv2d(conv2d, X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3af3821",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T05:41:02.127721Z",
     "start_time": "2021-12-05T05:41:02.112761Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用高为5、宽为3的卷积核。在高和宽两侧的填充数分别为2和1\n",
    "kernel_r = 5\n",
    "kernel_c=3\n",
    "conv2d = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=(5, 3), padding=(2, 1))\n",
    "comp_conv2d(conv2d, X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92c7d3d",
   "metadata": {},
   "source": [
    "## 步幅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4fe96be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T05:41:02.142681Z",
     "start_time": "2021-12-05T05:41:02.128718Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d = nn.Conv2d(1, 1, kernel_size=3, padding=1, stride=2)\n",
    "comp_conv2d(conv2d, X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219aa7c5",
   "metadata": {},
   "source": [
    "## 多输入通道和多输出通道"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a157d9b",
   "metadata": {},
   "source": [
    "### 多输入通道"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7704bd2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T05:41:02.157640Z",
     "start_time": "2021-12-05T05:41:02.143679Z"
    }
   },
   "outputs": [],
   "source": [
    "#多核卷积\n",
    "def corr2d_multi_in(X, K):\n",
    "    # 沿着X和K的第0维（通道维）分别计算再相加\n",
    "    res = corr2d(X[0, :, :], K[0, :, :])\n",
    "    for i in range(1, X.shape[0]):\n",
    "        res += corr2d(X[i, :, :], K[i, :, :])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8efd52fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T05:41:02.172629Z",
     "start_time": "2021-12-05T05:41:02.158638Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 56.,  72.],\n",
       "        [104., 120.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([[[0, 1, 2], [3, 4, 5], [6, 7, 8]],\n",
    "              [[1, 2, 3], [4, 5, 6], [7, 8, 9]]])\n",
    "K = torch.tensor([[[0, 1], [2, 3]], [[1, 2], [3, 4]]])\n",
    "\n",
    "corr2d_multi_in(X, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814b618c",
   "metadata": {},
   "source": [
    "### 多输出通道"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7db52e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T05:41:02.188069Z",
     "start_time": "2021-12-05T05:41:02.173600Z"
    }
   },
   "outputs": [],
   "source": [
    "def corr2d_multi_in_out(X, K):\n",
    "    # 对K的第0维遍历，每次同输入X做互相关计算。所有结果使用stack函数合并在一起\n",
    "    return torch.stack([corr2d_multi_in(X, k) for k in K])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99b04d5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T05:41:02.204026Z",
     "start_time": "2021-12-05T05:41:02.189066Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3, 3]), torch.Size([3, 2, 2, 2]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#核数组K同K+1（K中每个元素加一）和K+2连结在一起来构造一个输出通道数为3的卷积核。\n",
    "K = torch.stack([K, K + 1, K + 2])\n",
    "X.shape,K.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "44598eaa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T05:41:02.250899Z",
     "start_time": "2021-12-05T05:41:02.205022Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 56.,  72.],\n",
       "         [104., 120.]],\n",
       "\n",
       "        [[ 76., 100.],\n",
       "         [148., 172.]],\n",
       "\n",
       "        [[ 96., 128.],\n",
       "         [192., 224.]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr2d_multi_in_out(X, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a5c2ac",
   "metadata": {},
   "source": [
    "### 1x1卷积层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "969ad529",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T05:41:02.266857Z",
     "start_time": "2021-12-05T05:41:02.251897Z"
    }
   },
   "outputs": [],
   "source": [
    "def corr2d_multi_in_out_1x1(X, K):\n",
    "    c_i, h, w = X.shape\n",
    "    c_o = K.shape[0]\n",
    "    X = X.view(c_i, h * w)\n",
    "    K = K.view(c_o, c_i)\n",
    "    Y = torch.mm(K, X)  # 全连接层的矩阵乘法\n",
    "    return Y.view(c_o, h, w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1845a081",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T05:41:02.297281Z",
     "start_time": "2021-12-05T05:41:02.267854Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.rand(3, 3, 3)\n",
    "K = torch.rand(2, 3, 1, 1)\n",
    "\n",
    "Y1 = corr2d_multi_in_out_1x1(X, K)\n",
    "Y2 = corr2d_multi_in_out(X, K)\n",
    "\n",
    "(Y1 - Y2).norm().item() < 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83aa975",
   "metadata": {},
   "source": [
    "## 池化层\n",
    "它的提出是为了缓解卷积层对位置的过度敏感性"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360630f1",
   "metadata": {},
   "source": [
    "### 二维最大池化层和平均池化层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a41c0316",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T05:41:02.313240Z",
     "start_time": "2021-12-05T05:41:02.298279Z"
    }
   },
   "outputs": [],
   "source": [
    "def pool2d(X, pool_size, mode='max'):\n",
    "    X = X.float()\n",
    "    p_h,p_w = pool_size\n",
    "    Y = torch.zeros(X.shape[0]-p_h+1, X.shape[1]-p_w+1)\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            if mode == 'max':\n",
    "                Y[i,j] = X[i:i+p_h, j:j+p_w].max()\n",
    "            elif mode == 'avg':\n",
    "                Y[i,j] = X[i:i+p_h, j:j+p_w].mean()\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2050e20c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T05:41:02.329139Z",
     "start_time": "2021-12-05T05:41:02.314237Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4., 5.],\n",
       "        [7., 8.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([[0, 1, 2], [3, 4, 5], [6, 7, 8]])\n",
    "pool2d(X, (2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b8b67583",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T05:41:02.344103Z",
     "start_time": "2021-12-05T05:41:02.330138Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 3.],\n",
       "        [5., 6.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool2d(X, (2, 2), 'avg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561fd4eb",
   "metadata": {},
   "source": [
    "### 填充和步幅\n",
    "同卷积层一样，池化层也可以在输入的高和宽两侧的填充并调整窗口的移动步幅来改变输出形状。池化层填充和步幅与卷积层填充和步幅的工作机制一样。我们将通过nn模块里的二维最大池化层MaxPool2d来演示池化层填充和步幅的工作机制。我们先构造一个形状为(1, 1, 4, 4)的输入数据，前两个维度分别是批量和通道。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "46003c08",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T05:41:02.360062Z",
     "start_time": "2021-12-05T05:41:02.347092Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.,  1.,  2.,  3.],\n",
       "          [ 4.,  5.,  6.,  7.],\n",
       "          [ 8.,  9., 10., 11.],\n",
       "          [12., 13., 14., 15.]]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(16, dtype=torch.float).view((1, 1, 4, 4))\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1641dac4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T05:41:02.392009Z",
     "start_time": "2021-12-05T05:41:02.362052Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[10.]]]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#默认获得形状为(3, 3)的步幅。\n",
    "pool2d = nn.MaxPool2d(3)\n",
    "pool2d(X) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fdf7bb71",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T05:41:02.406932Z",
     "start_time": "2021-12-05T05:41:02.396959Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 5.,  7.],\n",
       "          [13., 15.]]]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool2d = nn.MaxPool2d(3, padding=1, stride=2)\n",
    "pool2d(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a2fdf387",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T05:41:02.422890Z",
     "start_time": "2021-12-05T05:41:02.408926Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.,  3.],\n",
       "          [ 9., 11.],\n",
       "          [13., 15.]]]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#也可以指定非正方形的池化窗口，并分别指定高和宽上的填充和步幅\n",
    "pool2d = nn.MaxPool2d((2, 4), padding=(1, 2), stride=(2, 3))\n",
    "pool2d(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "69799431",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T05:41:02.438847Z",
     "start_time": "2021-12-05T05:41:02.423888Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.,  1.,  2.,  3.],\n",
       "          [ 4.,  5.,  6.,  7.],\n",
       "          [ 8.,  9., 10., 11.],\n",
       "          [12., 13., 14., 15.]],\n",
       "\n",
       "         [[ 1.,  2.,  3.,  4.],\n",
       "          [ 5.,  6.,  7.,  8.],\n",
       "          [ 9., 10., 11., 12.],\n",
       "          [13., 14., 15., 16.]]]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#在处理多通道输入数据时，池化层对每个输入通道分别池化，而不是像卷积层那样将各通道的输入按通道相加\n",
    "X = torch.cat((X, X + 1), dim=1)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8caa6eed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T05:41:02.454804Z",
     "start_time": "2021-12-05T05:41:02.439844Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 5.,  7.],\n",
       "          [13., 15.]],\n",
       "\n",
       "         [[ 6.,  8.],\n",
       "          [14., 16.]]]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool2d = nn.MaxPool2d(3, padding=1, stride=2)\n",
    "pool2d(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741161e1",
   "metadata": {},
   "source": [
    "# 卷积神经网络(LeNet)  \n",
    "\n",
    "![](./images/LeNet.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f8713f14",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T05:50:31.295928Z",
     "start_time": "2021-12-05T05:50:31.289943Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 6, 5),# in_channels, out_channels, kernel_size\n",
    "            nn.Sigmoid(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            nn.Conv2d(6, 16, 5),\n",
    "            nn.Sigmoid(),\n",
    "            nn.MaxPool2d(2,2)\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(16 * 4 * 4, 120),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(84, 10)\n",
    "        )\n",
    "    def forward(self, img):\n",
    "        feature = self.conv(img)\n",
    "        output = self.fc(feature.view(img.shape[0], -1))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f1683b1c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T05:50:31.652635Z",
     "start_time": "2021-12-05T05:50:31.641637Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LeNet(\n",
       "  (conv): Sequential(\n",
       "    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): Sigmoid()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (4): Sigmoid()\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=120, bias=True)\n",
       "    (1): Sigmoid()\n",
       "    (2): Linear(in_features=120, out_features=84, bias=True)\n",
       "    (3): Sigmoid()\n",
       "    (4): Linear(in_features=84, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = LeNet()\n",
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6a1662",
   "metadata": {},
   "source": [
    "## 获取数据和训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3b4441de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T06:01:16.800672Z",
     "start_time": "2021-12-05T06:00:38.930522Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on  cuda\n",
      "epoch 1, loss 0.9649, train acc 0.626, test acc 0.676, time 7.6 sec\n",
      "epoch 2, loss 0.7860, train acc 0.704, test acc 0.720, time 7.5 sec\n",
      "epoch 3, loss 0.6890, train acc 0.736, test acc 0.743, time 7.6 sec\n",
      "epoch 4, loss 0.6301, train acc 0.754, test acc 0.757, time 7.5 sec\n",
      "epoch 5, loss 0.5952, train acc 0.767, test acc 0.769, time 7.6 sec\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "train_iter, test_iter = load_data_fashion_mnist(batch_size=batch_size)\n",
    "lr, num_epochs = 0.001, 5\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba308552",
   "metadata": {},
   "source": [
    "# 深度卷积神经网络（AlexNet）\n",
    "![](./images/AlexNet.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8665363b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T07:06:38.529681Z",
     "start_time": "2021-12-05T07:06:38.516716Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#简化过的AlexNet\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 96, 11, 4),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(3, 2),# kernel_size, stride\n",
    "            # in_channels, out_channels, kernel_size, stride, padding\n",
    "            nn.Conv2d(96, 256, 5, 1, 2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(3, 2),\n",
    "            nn.Conv2d(256, 384, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(384, 384, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(384, 256, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(3, 2)\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(256*5*5, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 10)\n",
    "        )\n",
    "    def forward(self, img):\n",
    "        feature = self.conv(img)\n",
    "        output = self.fc(feature.view(img.shape[0], -1))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8fd10f1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T07:06:38.860838Z",
     "start_time": "2021-12-05T07:06:38.658339Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (conv): Sequential(\n",
       "    (0): Conv2d(1, 96, kernel_size=(11, 11), stride=(4, 4))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(96, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU()\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU()\n",
       "    (8): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU()\n",
       "    (10): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU()\n",
       "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=6400, out_features=4096, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = AlexNet()\n",
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438fa27e",
   "metadata": {},
   "source": [
    "## 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "17caf69e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T07:06:39.096513Z",
     "start_time": "2021-12-05T07:06:39.049639Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "# 如出现“out of memory”的报错信息，可减小batch_size或resize\n",
    "train_iter, test_iter = load_data_fashion_mnist(batch_size, resize=224)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2f190d",
   "metadata": {},
   "source": [
    "## 训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d9f0d3f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T07:28:22.022544Z",
     "start_time": "2021-12-05T07:06:40.459594Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on  cuda\n",
      "epoch 1, loss 0.6026, train acc 0.767, test acc 0.865, time 263.0 sec\n",
      "epoch 2, loss 0.3222, train acc 0.880, test acc 0.873, time 270.3 sec\n",
      "epoch 3, loss 0.2804, train acc 0.896, test acc 0.896, time 249.4 sec\n",
      "epoch 4, loss 0.2552, train acc 0.906, test acc 0.900, time 261.4 sec\n",
      "epoch 5, loss 0.2319, train acc 0.915, test acc 0.911, time 257.4 sec\n"
     ]
    }
   ],
   "source": [
    "lr, num_epochs = 0.001, 5\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2473de5",
   "metadata": {},
   "source": [
    "# 使用重复元素的网络(VGG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b64ff9",
   "metadata": {},
   "source": [
    "## VGG块\n",
    "VGG块的组成规律是：连续使用数个相同的填充为1、窗口形状为3×3的卷积层后接上一个步幅为2、窗口形状为2×2的最大池化层。卷积层保持输入的高和宽不变，而池化层则对其减半。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6d6e5e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T07:44:57.614677Z",
     "start_time": "2021-12-06T07:44:57.595727Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def vgg_block(num_convs, in_channels, out_channels):\n",
    "    blk = []\n",
    "    for i in range(num_convs):\n",
    "        if i == 0:\n",
    "            blk.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))\n",
    "        else:\n",
    "            blk.append(nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1))\n",
    "        blk.append(nn.ReLU())\n",
    "    blk.append(nn.MaxPool2d(kernel_size=2, stride=2)) # 这里会使宽高减半\n",
    "    return nn.Sequential(*blk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7465406d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T07:44:57.751312Z",
     "start_time": "2021-12-06T07:44:57.739343Z"
    }
   },
   "outputs": [],
   "source": [
    "conv_arch = ((1, 1, 64), (1, 64, 128), (2, 128, 256), (2, 256, 512), (2, 512, 512))\n",
    "# 经过5个vgg_block, 宽高会减半5次, 变成 224/32 = 7\n",
    "fc_features = 512 * 7 * 7 # c * w * h\n",
    "fc_hidden_units = 4096 # 任意"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ad52d284",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T07:44:57.893519Z",
     "start_time": "2021-12-06T07:44:57.882549Z"
    }
   },
   "outputs": [],
   "source": [
    "#实现VGG-11\n",
    "def vgg(conv_arch, fc_features, fc_hidden_units=4096):\n",
    "    net = nn.Sequential()\n",
    "    # 卷积层部分\n",
    "    for i, (num_convs, in_channels, out_channels) in enumerate(conv_arch):\n",
    "        # 每经过一个vgg_block都会使宽高减半\n",
    "        net.add_module(\"vgg_block_\" + str(i+1), vgg_block(num_convs, in_channels, out_channels))\n",
    "    # 全连接层部分\n",
    "    net.add_module(\"fc\", nn.Sequential(FlattenLayer(),\n",
    "                                 nn.Linear(fc_features, fc_hidden_units),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Dropout(0.5),\n",
    "                                 nn.Linear(fc_hidden_units, fc_hidden_units),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Dropout(0.5),\n",
    "                                 nn.Linear(fc_hidden_units, 10)\n",
    "                                ))\n",
    "    return net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "39fe33ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T07:44:59.698695Z",
     "start_time": "2021-12-06T07:44:59.011537Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vgg_block_1 output shape:  torch.Size([1, 64, 112, 112])\n",
      "vgg_block_2 output shape:  torch.Size([1, 128, 56, 56])\n",
      "vgg_block_3 output shape:  torch.Size([1, 256, 28, 28])\n",
      "vgg_block_4 output shape:  torch.Size([1, 512, 14, 14])\n",
      "vgg_block_5 output shape:  torch.Size([1, 512, 7, 7])\n",
      "fc output shape:  torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "net = vgg(conv_arch, fc_features, fc_hidden_units)\n",
    "X = torch.rand(1,1,224,224)\n",
    "for name, blk in net.named_children():\n",
    "    X = blk(X)\n",
    "    print(name, 'output shape: ', X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7707d28c",
   "metadata": {},
   "source": [
    "## 获取数据和训练模型\n",
    "因为VGG-11计算上比AlexNet更加复杂，出于测试的目的我们构造一个通道数更小，或者说更窄的网络在Fashion-MNIST数据集上进行训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "87bcae21",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T07:45:02.430504Z",
     "start_time": "2021-12-06T07:45:02.410557Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (vgg_block_1): Sequential(\n",
      "    (0): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (vgg_block_2): Sequential(\n",
      "    (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (vgg_block_3): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (vgg_block_4): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (vgg_block_5): Sequential(\n",
      "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): FlattenLayer()\n",
      "    (1): Linear(in_features=3136, out_features=512, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Dropout(p=0.5, inplace=False)\n",
      "    (7): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "ratio = 8\n",
    "small_conv_arch = [(1, 1, 64//ratio), (1, 64//ratio, 128//ratio), (2, 128//ratio, 256//ratio), \n",
    "                   (2, 256//ratio, 512//ratio), (2, 512//ratio, 512//ratio)]\n",
    "net = vgg(small_conv_arch, fc_features // ratio, fc_hidden_units // ratio)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4ec36f97",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T07:56:25.748651Z",
     "start_time": "2021-12-06T07:45:03.083657Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on  cuda\n",
      "epoch 1, loss 0.5713, train acc 0.789, test acc 0.881, time 136.0 sec\n",
      "epoch 2, loss 0.3187, train acc 0.884, test acc 0.901, time 136.7 sec\n",
      "epoch 3, loss 0.2737, train acc 0.900, test acc 0.904, time 136.4 sec\n",
      "epoch 4, loss 0.2434, train acc 0.910, test acc 0.916, time 136.6 sec\n",
      "epoch 5, loss 0.2188, train acc 0.922, test acc 0.918, time 136.9 sec\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "train_iter, test_iter = load_data_fashion_mnist(batch_size, resize=224)\n",
    "\n",
    "lr, num_epochs = 0.001, 5\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d39d59b",
   "metadata": {},
   "source": [
    "# 网络中的网络（NiN）\n",
    "它提出了另外一个思路，即串联多个由卷积层和“全连接”层构成的小网络来构建一个深层网络。\n",
    "![](./images/NiN.svg)\n",
    "左图是AlexNet和VGG的网络结构局部，右图是NiN的网络结构局部"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e685d2e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T08:22:32.855527Z",
     "start_time": "2021-12-06T08:22:32.847547Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def nin_block(in_channels, out_channels, kernel_size, stride, padding):\n",
    "    blk = nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Conv2d(out_channels, out_channels, kernel_size=1),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Conv2d(out_channels, out_channels, kernel_size=1),\n",
    "                        nn.ReLU())\n",
    "    return blk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e34e529",
   "metadata": {},
   "source": [
    "除使用NiN块以外，NiN还有一个设计与AlexNet显著不同：NiN去掉了AlexNet最后的3个全连接层，取而代之地，NiN使用了输出通道数等于标签类别数的NiN块，然后使用全局平均池化层对每个通道中所有元素求平均并直接用于分类。这里的全局平均池化层即窗口形状等于输入空间维形状的平均池化层。NiN的这个设计的好处是可以显著减小模型参数尺寸，从而缓解过拟合。然而，该设计有时会造成获得有效模型的训练时间的增加。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8a2896",
   "metadata": {},
   "source": [
    "## NiN模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c3346193",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T08:46:00.315040Z",
     "start_time": "2021-12-06T08:46:00.282125Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class GlobalAvgPool2d(nn.Module):\n",
    "    # 全局平均池化层可通过将池化窗口形状设置成输入的高和宽实现\n",
    "    def __init__(self):\n",
    "        super(GlobalAvgPool2d, self).__init__()\n",
    "    def forward(self, x):\n",
    "        return F.avg_pool2d(x, kernel_size=x.size()[2:])\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nin_block(1, 96, kernel_size=11, stride=4, padding=0),\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "    nin_block(96, 256, kernel_size=5, stride=1, padding=2),\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "    nin_block(256, 384, kernel_size=3, stride=1, padding=1),\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2), \n",
    "    nn.Dropout(0.5),\n",
    "    # 标签类别数是10\n",
    "    nin_block(384, 10, kernel_size=3, stride=1, padding=1),\n",
    "    GlobalAvgPool2d(), \n",
    "    # 将四维的输出转成二维的输出，其形状为(批量大小, 10)\n",
    "    FlattenLayer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "844d7dd0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T08:46:02.653567Z",
     "start_time": "2021-12-06T08:46:02.632623Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 output shape:  torch.Size([1, 96, 54, 54])\n",
      "1 output shape:  torch.Size([1, 96, 26, 26])\n",
      "2 output shape:  torch.Size([1, 256, 26, 26])\n",
      "3 output shape:  torch.Size([1, 256, 12, 12])\n",
      "4 output shape:  torch.Size([1, 384, 12, 12])\n",
      "5 output shape:  torch.Size([1, 384, 5, 5])\n",
      "6 output shape:  torch.Size([1, 384, 5, 5])\n",
      "7 output shape:  torch.Size([1, 10, 5, 5])\n",
      "8 output shape:  torch.Size([1, 10, 1, 1])\n",
      "9 output shape:  torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(1, 1, 224, 224)\n",
    "for name, blk in net.named_children(): \n",
    "    X = blk(X)\n",
    "    print(name, 'output shape: ', X.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63e71ae",
   "metadata": {},
   "source": [
    "## 获取数据和训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9304dca5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T08:53:01.042418Z",
     "start_time": "2021-12-06T08:46:08.783947Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on  cuda\n",
      "epoch 1, loss 2.3027, train acc 0.099, test acc 0.100, time 412.2 sec\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "train_iter, test_iter = load_data_fashion_mnist(batch_size, resize=224)\n",
    "\n",
    "lr, num_epochs = 1, 1\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c7d92c",
   "metadata": {},
   "source": [
    "# 含并行连接的网络（GoogLeNet）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760e2721",
   "metadata": {},
   "source": [
    "# 批量归一化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf385b7f",
   "metadata": {},
   "source": [
    "## 从零开始实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ebd5356a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T09:03:08.737164Z",
     "start_time": "2021-12-06T09:03:08.723201Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3cf10dcb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T09:12:31.632457Z",
     "start_time": "2021-12-06T09:12:31.620488Z"
    }
   },
   "outputs": [],
   "source": [
    "def batch_norm(is_training, X, gamma, beta, moving_mean, moving_var, eps, momentum):\n",
    "    if not is_training:\n",
    "        X_hat = (X-moving_mean)/torch.sqrt(moving_var + eps)\n",
    "    else:\n",
    "        assert len(X.shape) in (2, 4)\n",
    "        if len(X.shape) == 2:\n",
    "            #使用全连接\n",
    "            mean = X.mean(dim=0)\n",
    "            var = ((X-mean)**2).mean(dim=0)\n",
    "        else:\n",
    "            #使用二维卷积层，计算通道维上（axis=1）的均值和方差\n",
    "            mean = X.mean(dim=0, keepdim=True).mean(dim=2,keepdim=True).mean(dim=3, keepdim=True)\n",
    "            var = ((X - mean) ** 2).mean(dim=0, keepdim=True).mean(dim=2, keepdim=True).mean(dim=3, keepdim=True)\n",
    "        #训练模式下做标准化\n",
    "        X_hat = (X-mean) / torch.sqrt(var+eps)\n",
    "        \n",
    "        moving_mean = momentum * moving_mean + (1-momentum)* mean\n",
    "        moving_var = momentum*moving_var + (1-momentum)*var\n",
    "    Y = gamma * X_hat + beta\n",
    "    return Y, moving_mean, moving_var\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cff9e5",
   "metadata": {},
   "source": [
    "我们自定义一个BatchNorm层。它保存参与求梯度和迭代的拉伸参数gamma和偏移参数beta，同时也维护移动平均得到的均值和方差，以便能够在模型预测时被使用。BatchNorm实例所需指定的num_features参数对于全连接层来说应为输出个数，对于卷积层来说则为输出通道数。该实例所需指定的num_dims参数对于全连接层和卷积层来说分别为2和4。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1d3abf62",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T09:12:31.913735Z",
     "start_time": "2021-12-06T09:12:31.899774Z"
    }
   },
   "outputs": [],
   "source": [
    "class BatchNorm(nn.Module):\n",
    "    def __init__(self, num_features, num_dims):\n",
    "        super(BatchNorm, self).__init__()\n",
    "        if num_dims == 2:\n",
    "            shape = (1, num_features)\n",
    "        else:\n",
    "            shape = (1, num_features, 1, 1)\n",
    "        # 参与求梯度和迭代的拉伸和偏移参数，分别初始化成0和1\n",
    "        self.gamma = nn.Parameter(torch.ones(shape))\n",
    "        self.beta = nn.Parameter(torch.zeros(shape))\n",
    "        # 不参与求梯度和迭代的变量，全在内存上初始化成0\n",
    "        self.moving_mean = torch.zeros(shape)\n",
    "        self.moving_var = torch.zeros(shape)  \n",
    "    \n",
    "    def forward(self, X):\n",
    "        if self.moving_mean.device != X.device:\n",
    "            self.moving_mean = self.moving_mean.to(X.device)\n",
    "            self.moving_var = self.moving_var.to(X.device)\n",
    "        Y, self.moving_mean, self.moving_var = batch_norm(self.training, \n",
    "            X, self.gamma, self.beta, self.moving_mean,\n",
    "            self.moving_var, eps=1e-5, momentum=0.9)\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e15214b",
   "metadata": {},
   "source": [
    "## 使用批量归一化层的LeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "20d7935a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T09:12:32.194115Z",
     "start_time": "2021-12-06T09:12:32.179156Z"
    }
   },
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "            nn.Conv2d(1, 6, 5), # in_channels, out_channels, kernel_size\n",
    "            BatchNorm(6, num_dims=4),\n",
    "            nn.Sigmoid(),\n",
    "            nn.MaxPool2d(2, 2), # kernel_size, stride\n",
    "            nn.Conv2d(6, 16, 5),\n",
    "            BatchNorm(16, num_dims=4),\n",
    "            nn.Sigmoid(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            FlattenLayer(),\n",
    "            nn.Linear(16*4*4, 120),\n",
    "            BatchNorm(120, num_dims=2),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(120, 84),\n",
    "            BatchNorm(84, num_dims=2),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(84, 10)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4c0ff5be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T09:13:19.650054Z",
     "start_time": "2021-12-06T09:12:32.884355Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on  cuda\n",
      "epoch 1, loss 1.0114, train acc 0.781, test acc 0.834, time 9.1 sec\n",
      "epoch 2, loss 0.4627, train acc 0.862, test acc 0.848, time 9.1 sec\n",
      "epoch 3, loss 0.3690, train acc 0.878, test acc 0.847, time 9.2 sec\n",
      "epoch 4, loss 0.3339, train acc 0.885, test acc 0.858, time 9.5 sec\n",
      "epoch 5, loss 0.3100, train acc 0.891, test acc 0.868, time 9.8 sec\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "train_iter, test_iter = load_data_fashion_mnist(batch_size=batch_size)\n",
    "\n",
    "lr, num_epochs = 0.001, 5\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f68ce122",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T09:13:31.466894Z",
     "start_time": "2021-12-06T09:13:31.413032Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.0538, 1.0255, 1.1623, 1.0720, 1.0058, 0.9147], device='cuda:0',\n",
       "        grad_fn=<ViewBackward>),\n",
       " tensor([-0.1301,  0.0880,  0.2942, -0.6030, -0.6827, -0.5257], device='cuda:0',\n",
       "        grad_fn=<ViewBackward>))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[1].gamma.view((-1,)), net[1].beta.view((-1,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8c0b5e",
   "metadata": {},
   "source": [
    "## 简洁实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8092c2cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T09:14:20.032122Z",
     "start_time": "2021-12-06T09:14:19.972055Z"
    }
   },
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "            nn.Conv2d(1, 6, 5), # in_channels, out_channels, kernel_size\n",
    "            nn.BatchNorm2d(6),\n",
    "            nn.Sigmoid(),\n",
    "            nn.MaxPool2d(2, 2), # kernel_size, stride\n",
    "            nn.Conv2d(6, 16, 5),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.Sigmoid(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            FlattenLayer(),\n",
    "            nn.Linear(16*4*4, 120),\n",
    "            nn.BatchNorm1d(120),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.BatchNorm1d(84),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(84, 10)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c6e8f085",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T09:15:11.456026Z",
     "start_time": "2021-12-06T09:14:32.925104Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on  cuda\n",
      "epoch 1, loss 1.0033, train acc 0.789, test acc 0.817, time 7.8 sec\n",
      "epoch 2, loss 0.4608, train acc 0.863, test acc 0.807, time 7.6 sec\n",
      "epoch 3, loss 0.3665, train acc 0.880, test acc 0.867, time 7.7 sec\n",
      "epoch 4, loss 0.3318, train acc 0.888, test acc 0.825, time 7.7 sec\n",
      "epoch 5, loss 0.3102, train acc 0.893, test acc 0.836, time 7.7 sec\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "train_iter, test_iter = load_data_fashion_mnist(batch_size=batch_size)\n",
    "\n",
    "lr, num_epochs = 0.001, 5\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9e2ce5",
   "metadata": {},
   "source": [
    "在模型训练时，批量归一化利用小批量上的均值和标准差，不断调整神经网络的中间输出，从而使整个神经网络在各层的中间输出的数值更稳定。\n",
    "对全连接层和卷积层做批量归一化的方法稍有不同。\n",
    "批量归一化层和丢弃层一样，在训练模式和预测模式的计算结果是不一样的。\n",
    "PyTorch提供了BatchNorm类方便使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ac5378",
   "metadata": {},
   "source": [
    "# 残差网络（ResNet）\n",
    "![](./images/ResNet.svg)\n",
    "普通的网络结构（左）与加入残差连接的网络结构（右）\n",
    "ResNet沿用了VGG全3×3卷积层的设计。残差块里首先有2个有相同输出通道数的3×3卷积层。每个卷积层后接一个批量归一化层和ReLU激活函数。然后我们将输入跳过这两个卷积运算后直接加在最后的ReLU激活函数前。这样的设计要求两个卷积层的输出与输入形状一样，从而可以相加。如果想改变通道数，就需要引入一个额外的1×1卷积层来将输入变换成需要的形状后再做相加运算。\n",
    "\n",
    "残差块的实现如下。它可以设定输出通道数、是否使用额外的1×1卷积层来修改通道数以及卷积层的步幅。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "23ce7c4a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T09:31:38.777920Z",
     "start_time": "2021-12-06T09:31:38.770938Z"
    }
   },
   "outputs": [],
   "source": [
    "class Residual(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, use_1x1conv=False, stride=1):\n",
    "        super(Residual, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, stride=stride)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        if use_1x1conv:\n",
    "            self.conv3 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride)\n",
    "        else:\n",
    "            self.conv3 = None\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "    def forward(self, X):\n",
    "        Y = F.relu(self.bn1(self.conv1(X)))\n",
    "        Y = self.bn2(self.conv2(Y))\n",
    "        if self.conv3:\n",
    "            X = self.conv3(X)\n",
    "        return F.relu(Y+X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2c301be0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T09:31:47.579082Z",
     "start_time": "2021-12-06T09:31:47.566117Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 6, 6])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blk = Residual(3, 3)\n",
    "X = torch.rand((4, 3, 6, 6))\n",
    "blk(X).shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e5bcdaf6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T09:32:07.329707Z",
     "start_time": "2021-12-06T09:32:07.294801Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 6, 3, 3])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#也可以在增加输出通道数的同时减半输出的高和宽\n",
    "blk = Residual(3, 6, use_1x1conv=True, stride=2)\n",
    "blk(X).shape "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87baf814",
   "metadata": {},
   "source": [
    "## ResNet模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bbbda4dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T09:34:23.026780Z",
     "start_time": "2021-12-06T09:34:23.014256Z"
    }
   },
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c728a05b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T09:35:09.842742Z",
     "start_time": "2021-12-06T09:35:09.830774Z"
    }
   },
   "outputs": [],
   "source": [
    "def resnet_block(in_channels, out_channels, num_residuals, first_block=False):\n",
    "    if first_block:\n",
    "        assert in_channels == out_channels # 第一个模块的通道数同输入通道数一致\n",
    "    blk = []\n",
    "    for i in range(num_residuals):\n",
    "        if i == 0 and not first_block:\n",
    "            blk.append(Residual(in_channels, out_channels, use_1x1conv=True, stride=2))\n",
    "        else:\n",
    "            blk.append(Residual(out_channels, out_channels))\n",
    "    return nn.Sequential(*blk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "377f6443",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T09:35:13.804609Z",
     "start_time": "2021-12-06T09:35:13.741773Z"
    }
   },
   "outputs": [],
   "source": [
    "net.add_module(\"resnet_block1\", resnet_block(64, 64, 2, first_block=True))\n",
    "net.add_module(\"resnet_block2\", resnet_block(64, 128, 2))\n",
    "net.add_module(\"resnet_block3\", resnet_block(128, 256, 2))\n",
    "net.add_module(\"resnet_block4\", resnet_block(256, 512, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "261ee7d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T09:35:30.600787Z",
     "start_time": "2021-12-06T09:35:30.583832Z"
    }
   },
   "outputs": [],
   "source": [
    "net.add_module(\"global_avg_pool\", GlobalAvgPool2d())\n",
    "net.add_module(\"fc\", nn.Sequential(FlattenLayer(), nn.Linear(512, 10))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "65802731",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T09:35:44.261774Z",
     "start_time": "2021-12-06T09:35:44.199159Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  output shape:\t torch.Size([1, 64, 112, 112])\n",
      "1  output shape:\t torch.Size([1, 64, 112, 112])\n",
      "2  output shape:\t torch.Size([1, 64, 112, 112])\n",
      "3  output shape:\t torch.Size([1, 64, 56, 56])\n",
      "resnet_block1  output shape:\t torch.Size([1, 64, 56, 56])\n",
      "resnet_block2  output shape:\t torch.Size([1, 128, 28, 28])\n",
      "resnet_block3  output shape:\t torch.Size([1, 256, 14, 14])\n",
      "resnet_block4  output shape:\t torch.Size([1, 512, 7, 7])\n",
      "global_avg_pool  output shape:\t torch.Size([1, 512, 1, 1])\n",
      "fc  output shape:\t torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "# 训练ResNet之前，我们来观察一下输入形状在ResNet不同模块之间的变化。\n",
    "X = torch.rand((1, 1, 224, 224))\n",
    "for name, layer in net.named_children():\n",
    "    X = layer(X)\n",
    "    print(name, ' output shape:\\t', X.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a70363",
   "metadata": {},
   "source": [
    "## 获取数据和训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f5f9bdcd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T09:47:31.348760Z",
     "start_time": "2021-12-06T09:36:37.016007Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on  cuda\n",
      "epoch 1, loss 0.4048, train acc 0.850, test acc 0.894, time 133.3 sec\n",
      "epoch 2, loss 0.2452, train acc 0.910, test acc 0.904, time 132.1 sec\n",
      "epoch 3, loss 0.2077, train acc 0.924, test acc 0.911, time 130.6 sec\n",
      "epoch 4, loss 0.1791, train acc 0.934, test acc 0.917, time 129.1 sec\n",
      "epoch 5, loss 0.1548, train acc 0.943, test acc 0.900, time 129.1 sec\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "# 如出现“out of memory”的报错信息，可减小batch_size或resize\n",
    "train_iter, test_iter = load_data_fashion_mnist(batch_size, resize=96)\n",
    "\n",
    "lr, num_epochs = 0.001, 5\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc680dd6",
   "metadata": {},
   "source": [
    "# 稠密连接网络（DenseNet）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a51f97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 288.85436400000003,
   "position": {
    "height": "40px",
    "left": "663.998px",
    "right": "20px",
    "top": "168.996px",
    "width": "462.008px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
