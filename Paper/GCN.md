+ ***[Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering(GCN)](https://proceedings.neurips.cc/paper/2016/hash/04df4d434d481c5bb723be1b6df1ee65-Abstract.html)***   

![](https://paperrecord.oss-cn-shanghai.aliyuncs.com/202205141542274.PNG)

**年份**：2016  

**引用次数**：3950  

**应用领域**：GNN  

**方法及优缺点**：

本模型旨在将CNN推广到图这种数据结构中, 在spectral graph理论的基础上使用CNN, 这种办法有和CNN一样的计算复杂度和学习复杂性, 并且这种办法可以适用于任何一种图结构中.

**结论**：

本文通过GSP工具将CNN推广到图中, 并且这种图卷积有能力提取局部和固定的特征

(i)作者引入了一个计算复杂度与数据维度呈线性关系的模型

(ii)我们证实了输入图的质量是最重要的

(iii)只要图构造良好，该模型提出的局部静止性和构成性的统计假设对文本文档来说是可以验证的

后续希望能引入外部信息,另一项后续是交替学习CNN的参数和图.

**动机**:  

CNN可以在大规模高维数据集中提取patterns, CNN最重要的特性是平移不变性, 这种特性使得它在处理不同空间的相同特征时,结果是一致的. 但是图一般都是non-Euclidean空间,无法保持平移不变性, 简单来说就是没有一个统一大小的卷积核可以对图进行卷积.所以本文就希望使用sprctral graph theory将CNN的卷积操作推广到图中.实现

1. 谱的设计
2. 能像CNN一样抓取局部的特征
3. 更低的时间复杂度,避免使用傅里叶变化的基
4. 高效的图粗化
5. 好的实验结果

**相关工作和理论**：

在图上的卷积分为两种办法, 第一种是在空间上,第二种是在频域中, 第一种空间上可能会遇到的问题有,  邻居数量不一致的问题, 并且从数学角度来看, 空间上的卷积没有专门的数学定义.而在频域中的卷积则有数学公式.

总体来说spectral的卷积分为4步

1. 将图通过傅里叶变换转到频域.
2. 将卷积核通过傅里叶变换转到频域
3. 将上述两个信号做乘法就完成了卷积操作
4. 将卷积结果通过反傅里叶变换转会到时域中

首先对于第一步中傅里叶变换,让一个函数变成无数个sin和cos的相加的结果, 变换公式如下,其中$e^{-i\omega x}$可以通过欧拉公式和三角函数建立联系, 但是其中要求$\underset{x->\infty}{f(x)}=0$这有它的缺陷, 比如$y=x^2$这种函数都无法实现傅里叶变换

![](https://paperrecord.oss-cn-shanghai.aliyuncs.com/202205141607263.svg)

所以将傅里叶变换做推广得到了拉普拉斯变换, 原理就是f(x)乘以$e^{-\beta x}$是可以让极限趋于0, 并且由于这个额外乘的内容可以和后面的有界量合并在一起, 所以就可以得到如下公式. 如果下线是$-\infty$的话就是双边拉普拉斯

![](https://paperrecord.oss-cn-shanghai.aliyuncs.com/202205141614805.svg)

综上, 由于拉普拉斯是傅里叶变换的推广, 适用性更好, 所以GCN中的傅里叶变换都是使用的拉普拉斯变换完成的.但是这是从傅里叶的缺陷角度得到的, 拉普拉斯和图的关系是什么.

所以将拉普拉斯变换从数学角度看是函数的梯度的散度, 可视化结果如下图, 可以将下面的向量箭头想成是一个图中vertice, 其中一个点的更新会根据周围点的运动方向做出调整

<img src="https://paperrecord.oss-cn-shanghai.aliyuncs.com/202205141619902.PNG" style="zoom:50%;" />

将这种思想推广到离散的图中, 举一个简单的例子就是将周围点和当前点的差值求和在一起可以当成当前点的该变量, $\Delta f_i=\sum_{j\in N_i}(f_i-f_j)$, 为了扩展公式将$N_i$邻居扩展到全图, 所以使用一个权重系数, 0为未连接, 1为连接,得到$\Delta f_i=\sum_{j\in N}w_{ij}(f_i-f_j)$继续推导得到
$$
\Delta f_i=\sum_{j\in N}w_{ij}(f_i-f_j) \\
=\sum_{j\in N}w_{ij}f_i-\sum_{j\in N}w_{ij}f_j\\
=d_if_i-a_if_i
$$
所以对于所有顶点的$\Delta f$有$\Delta f=(D-A)f$,其中D为度数矩阵, A为邻接矩阵. 所以拉普拉斯的矩阵定义出来就是$L=D-A$, 如果要计算改变量就计算$Lf$即可. 

知道了拉普拉斯矩阵, 那怎么做拉普拉斯变换, 又或者说怎么做傅里叶变换呢, 将傅里叶变换离散化, 求积分变成求和, 再引入线性代数的知识可以知道傅里叶公式可以写成如下形式. 所谓傅里叶变换可以看成是在一个$\{e^{i\omega n}\}_{n=\infty}$无穷基下的线性组合, $c_n$为系数. 同理拉普拉斯变换也可以写成是在$\{e^{sx}\}_{n=\infty}$这个基下的线性组合, 并且这两个基是完全一致的. 那对于一个其他向量空间中的向量, 只需要乘以基即可得到在傅里叶空间中的新向量.
$$
F(\omega)=\sum_{n=-\infty}^\infty c_ne^{i\omega n}
$$
那综上可以知道, 我们有了拉普拉斯矩阵, 我们只需要求出它的基即可, 那就进行相似对角化即可, 又由于$D-A$得到的是一个实对称矩阵, 所以拉普拉斯矩阵的分解有许多很好的特性, 比如必能有n个不相关的基, 特征向量一定大于0, 并且特征向量$U^T=U^{-1}$.这些性质都会在后面对计算有帮助. 分解拉普拉斯变换得到的结果如下, 其中U的每一列则为对应的拉普拉斯变换的基, $\lambda$可以看成是频域钟的频率.	

![](https://paperrecord.oss-cn-shanghai.aliyuncs.com/202205141649358.svg)

接着回到本文的GCN中, 我们通过度数矩阵和邻接矩阵得到了拉普拉斯矩阵$L=D-A$后进行相似对角化后得到$L=U\lambda U^T$后, 我们先对图做拉普拉斯变换, 具体做法就是把原来的图信号乘以$U$中每一个基,得到频域中的信号.公式就是$\hat{f}=U^Tf$, 那其实后续的反傅里叶变换就是把在频域中的向量,乘以$U^{-1}=U^T$转回时域中.

所谓卷积核在时域中也是一个矩阵, 也可以通过乘以拉普拉斯的基得到一个频域中的矩阵, 为了和图信号的对角矩阵一样, 所以卷积核在频域中也是以对角矩阵的形式存在, 如下图所示

![](https://paperrecord.oss-cn-shanghai.aliyuncs.com/202205141701390.svg)

再结合再频域中的卷积就是相乘的理论得到如下卷积公式,其中$f$为时域中图信号, $h$为时域中的卷积核
$$
(f * h)_G=U((U^Tf)\odot(U^Th))
$$
为了简化公式, 将卷积核在频域中对角矩阵的形式改写成$U^Th\to g_\theta (\Lambda)$这样就能修改Eq3为如下形式
$$
f' = Ug_\theta (\Lambda)U^Tf=g_\theta(L)f \\g_\theta (\Lambda)=diag(\theta)
$$
至此得到前人的工作, 类似CNN的卷积核的学习, 在GCN中只需要学习这个对角矩阵$diag(\theta)$即可, 但是这个有几个问题, 就是没有限制条件, 计算复杂度高, 如果图加一个节点就需要重新计算拉普拉斯矩阵重新学习, 而且这种办法的卷积是对整张图进行卷积, 没有CNN卷积局部特征的特性,所以前人想让卷积核在拉普拉斯相似对角化后的对角矩阵的基础上学习, 来减少学习量, 其次为了得到局部特征使用了k-hop的方法, 那前人的具体做法就是设计
$$
g_{\theta}(\Lambda) = \sum_{k=0}^{K-1}\theta_k\Lambda^k
$$
将这个公式带入到Eq4中得到, 可以看到计算的是拉普拉斯矩阵的$k$次方的结果, 这放在线性代数里面的意思就是k跳邻居. 而且这种计算的时间复杂度就是这个超参数$O(K)$, 而且直接计算拉普拉斯公式就行, 不需要矩阵分解的结果
$$
Ug_\theta (\Lambda)U^T=U\sum_{k=0}^{K-1}\theta_k\Lambda^kU^T=\sum_{k=0}^{K-1}\theta_kU\Lambda^kU^T=\sum_{k=0}^{K-1}\theta_kL^k
$$
本文作者为了在前人的基础上进一步优化结果, 引入了GSP中的切比雪夫公式, 具体卷积核设计为如下形式, 使用$\tilde\Lambda$因为切比雪夫多项式的定义域要求[-1, 1] ,所以使用2x[0,1]-1得到, $\lambda_{max}$可以通过幂迭代法得到.
$$
g_{\theta}(\Lambda) = \sum_{k=0}^{K-1}\theta_kT_k(\tilde\Lambda) \\
Ug_\theta (\Lambda)U^T=U\sum_{k=0}^{K-1}\theta_kT_k(\tilde\Lambda)U^T=\sum_{k=0}^{K-1}\theta_kT_k(U\tilde\Lambda U^T)=\sum_{k=0}^{K-1}\theta_kT_k(\tilde L)\\
T_k(x)=2xT_{j-1}(x)-T_{j-2}(x),T_0=1, T_1=x \\
\tilde\Lambda=2\Lambda/\lambda_{max}-I_n, \tilde L=2L/\lambda_{max}-I_n
$$
使用切比雪夫的好处是在之前的基础上连矩阵的k次都不需要计算, 直接通过切比雪夫多项式递归计算即可. 至此动机中的1-3都已经完成, 后续就是介绍Graph Coarsening.

所谓图粗化其实可以看成是readout操作, 就是将整个graph生成一个特征表示,这一块的论文理论不太理解, 还没入门, 需要的话以后回来补这一块的内容, 找到的一篇相关博客在备注.

**实验结果**:  

复现回来补

**个人总结**：  

花了几天把几个视频和博客还有一些数学科普博客都串了一遍算是学会了GCN的操作了, 但是扫读了几篇GNN推荐系统的论文,发现好像大部分都是spatial的, 感觉注意力要放在GraphSAGE和GAT这两篇论文中.

**备注**  

[从图(Graph)到图卷积(Graph Convolution): 漫谈图神经网络 (三)](https://zhuanlan.zhihu.com/p/108299847)